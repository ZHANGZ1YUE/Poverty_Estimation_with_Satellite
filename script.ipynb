{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_dataset = '/home/ziyue/zhang/resnet/cnn_ceni_phi_2015_train90.csv'\n",
    "\n",
    "df = pd.read_csv(train_dataset)\n",
    "df = df.dropna(subset=['POV_2015'])\n",
    "\n",
    "# Set the id = rownumber as index of the DataFrame\n",
    "id_to_pov = df.set_index('id')['POV_2015'].to_dict()\n",
    "print(id_to_pov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "os.makedirs('data', exist_ok=True)\n",
    "tar_file = '/home/ziyue/zhang/resnet/CNN_IMGB_PHI_2015_ST_384_JPG_3840.tar.gz'\n",
    "imagery_path = '/home/ziyue/zhang/resnet/data'\n",
    "#shutil.unpack_archive(tar_file, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "matched_files = {}\n",
    "\n",
    "jpg_files = glob.glob1(imagery_path,\"*.jpg\")\n",
    "print(\"Total images are \" + str(len(jpg_files)))\n",
    "for file in jpg_files:\n",
    "    file_id = int(file.split('_')[-1].split('.')[0])\n",
    "\n",
    "    if file_id in id_to_pov:\n",
    "            # Match the jpg file with the corresponding pov value\n",
    "            matched_files[file] = id_to_pov[file_id]\n",
    "\n",
    "print(\"Train dataset has \" + str(len(matched_files)) + \" data pairs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, matched_files, img_dir, transform=None):\n",
    "        self.matched_files = matched_files\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.matched_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, target_value = list(self.matched_files.items())[idx]\n",
    "        img_path = os.path.join(self.img_dir, filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        target_value = torch.tensor([target_value], dtype=torch.float32)\n",
    "        return image, target_value\n",
    "\n",
    "# Define any necessary image transformations\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),  # Resize the image (optional)\n",
    "    transforms.ToTensor(),  # Convert image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image (optional, but recommended for most pre-trained models)\n",
    "])\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dataset = CustomDataset(matched_files=matched_files, img_dir='/home/ziyue/zhang/resnet/data', transform=transform)\n",
    "\n",
    "\n",
    "val_percent: float = 0.2 #Use 20% as validation dataset\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "n_val = int(len(dataset) * val_percent)   #这几行就是定义多少个training 多少个validation\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "\n",
    "#Dataloader了该 Dataloader的作用就是打包Batch\n",
    "loader_args = dict(batch_size=batch_size, num_workers=4, pin_memory=True) #为下面的两行做准备\n",
    "train_loader = DataLoader(train_set, shuffle=True, **loader_args)   #dataloader来load这个dataset，分为train和validation\n",
    "val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
    "\n",
    "\n",
    "dataloaders = {\n",
    "  'train': train_loader,\n",
    "  'val': val_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import time\n",
    "\n",
    "# Define the model such that it takes an image and output a single value\n",
    "\n",
    "model = models.resnet50()\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "torch.cuda.set_device(7)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
    "\n",
    "def calc_loss(pred, target, metrics):\n",
    "    loss = criterion(pred, target)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
    "\n",
    "checkpoint_path = \"checkpoint.pth\"\n",
    "def train_model(model, optimizer, num_epochs):\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:  #train和val来回颠倒\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "\n",
    "            for input, output in dataloaders[phase]:\n",
    "                input = input.to(device, dtype=torch.float32)  #这里不加dtype的话，原来的tensor其实是double格式的\n",
    "                output = output.to(device, dtype=torch.float32)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    prediction = model(input)\n",
    "                    #print(\"prediction: \"+str(prediction))\n",
    "                    #print(\"output: \"+str(+output))\n",
    "                    #print(output)\n",
    "                    loss = calc_loss(prediction, output, metrics)\n",
    "                    #print(loss)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += input.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            #if phase == 'train':\n",
    "            #  scheduler.step()\n",
    "            #  for param_group in optimizer.param_groups:\n",
    "            #      print(\"LR\", param_group['lr'])\n",
    "\n",
    "            # save the model weights\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(f\"saving best model to {checkpoint_path}\")\n",
    "                best_loss = epoch_loss\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_model(model, optimizer, num_epochs = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raingan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
